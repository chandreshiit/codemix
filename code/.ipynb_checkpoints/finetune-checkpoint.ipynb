{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BowPDPgUJE7",
    "outputId": "4f1ccd64-139d-4733-c6da-f18e4be36163"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "# !pip install transformers[sentencepiece]\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "qGOLz_UqaQEH",
    "outputId": "ea5877b0-765c-4a4a-e684-20f1d44056a4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlikPGJnNBPH"
   },
   "outputs": [],
   "source": [
    "# pos=pd.read_csv('/content/drive/MyDrive/Added new data 2/positive_add1and2.csv')\n",
    "# neg=pd.read_csv('/content/drive/MyDrive/Added new data 2/negative_add_1and2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXA6NWafNaiu"
   },
   "outputs": [],
   "source": [
    "# pos=pos.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWt5D9iCOBYO"
   },
   "outputs": [],
   "source": [
    "# neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yCZKnDQNbv1"
   },
   "outputs": [],
   "source": [
    "# n=[]\n",
    "# for i in range(0,115207):\n",
    "#   n.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AW7U1mzrNlTO"
   },
   "outputs": [],
   "source": [
    "# neg['labels']=n\n",
    "# neg=neg.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFN1PJT6RfKL"
   },
   "outputs": [],
   "source": [
    "# neg=neg[0:83000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zuo4-QE8N6Ql"
   },
   "outputs": [],
   "source": [
    "# df=neg.append(pos,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-0_FdPwOdXb"
   },
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_H30QmQdOxUQ"
   },
   "outputs": [],
   "source": [
    "# h=[]\n",
    "# for i in df.tweet.values:\n",
    "#  h.append(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c4_Y63M8Xn9"
   },
   "outputs": [],
   "source": [
    "# df['tweets']=h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3dsQlLtSnFi"
   },
   "outputs": [],
   "source": [
    "# df=df.drop('tweet',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyd8dxjjQZ7t"
   },
   "outputs": [],
   "source": [
    "# sub_df=df[0:25000]\n",
    "# train=sub_df[0:20000]\n",
    "# test=sub_df[20000:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2wwJn3z9JMw"
   },
   "outputs": [],
   "source": [
    "# df.to_csv('/content/drive/MyDrive/Train and valid spli/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Fl0VHSdPeX-"
   },
   "outputs": [],
   "source": [
    "# df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pssOlRkUS9ln"
   },
   "outputs": [],
   "source": [
    "# train.to_csv('/content/drive/MyDrive/Train and valid spli/train.csv')\n",
    "# test.to_csv('/content/drive/MyDrive/Train and valid spli/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sH6WZbFlnkk"
   },
   "source": [
    "**XLM-Roberta Casual Language modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnHRAy7GTMKD"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files={'train':'/content/drive/MyDrive/dataset/Train and valid spli/train.csv','test':'/content/drive/MyDrive/dataset/Train and valid spli/test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffKqHu_jmc_N"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAn9TAg6UFJ1"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "tokenizer =XLMRobertaTokenizerFast.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnP_whvuUosx"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"tweets\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mC6m1Fr5Ur7B"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=2, remove_columns=[\"tweets\",\"Unnamed: 0\",\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVJhvfTe-scB"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAZNHtUUVDch"
   },
   "outputs": [],
   "source": [
    "block_size=128\n",
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S426_blbVH1X"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CBe6HXWVKA3"
   },
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkzM9ZbKsTmd"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVg5WIkbsPcW"
   },
   "outputs": [],
   "source": [
    "lm_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8adys3HXPZut"
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaForCausalLM\n",
    "model = XLMRobertaForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b109QD-_VimQ"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=4.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpGU_bWtWBNK"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElYjJqQOWFl2"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4y8qjkxWJSa"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANpTG7vwI9bQ"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EY0wOQ9Z4Qqg"
   },
   "outputs": [],
   "source": [
    "torch.save(model,'/content/drive/MyDrive/Model/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSVb1U53JX0R"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "new_model=torch.load('/content/drive/MyDrive/Model/model.pt',map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtIy_T0ug6R2"
   },
   "source": [
    "**XLM-Roberta Masked Language modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsuvLNCCco9T"
   },
   "outputs": [],
   "source": [
    "# from transformers import XLMRobertaForMaskedLM\n",
    "# model2 =XLMRobertaForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEHcOemhklDy"
   },
   "outputs": [],
   "source": [
    "# dataset = load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Train and valid spli/train.csv','valid':'/content/drive/MyDrive/Train and valid spli/valid.csv','test':'/content/drive/MyDrive/Train and valid spli/test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1utl17ok01P"
   },
   "outputs": [],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"tweets\"])\n",
    "# block_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzYPdGiElD_s"
   },
   "outputs": [],
   "source": [
    "# def group_texts(examples):\n",
    "#     # Concatenate all texts.\n",
    "#     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "#         # customize this part to your needs.\n",
    "#     total_length = (total_length // block_size) * block_size\n",
    "#     # Split by chunks of max_len.\n",
    "#     result = {\n",
    "#         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9TSTFOLk4tP"
   },
   "outputs": [],
   "source": [
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"tweets\",\"Unnamed: 0\",\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLSTl9oKk5Wk"
   },
   "outputs": [],
   "source": [
    "# lm_datasets = tokenized_datasets.map(\n",
    "#     group_texts,\n",
    "#     batched=True,\n",
    "#     batch_size=1000,\n",
    "#     num_proc=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM79NpUThWl_"
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     \"test-clm\",\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     learning_rate=2e-3,\n",
    "#     weight_decay=0.01,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68rQMXPMhZ36"
   },
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForLanguageModeling\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTGjLyUPhb2U"
   },
   "outputs": [],
   "source": [
    "# trainer2 = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=lm_datasets[\"train\"],\n",
    "#     eval_dataset=lm_datasets[\"valid\"],\n",
    "#     data_collator=data_collator,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOIuhhEMhfQ5"
   },
   "outputs": [],
   "source": [
    "# trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35_zR4YulTmF"
   },
   "outputs": [],
   "source": [
    "# import math\n",
    "# eval_results = trainer2.evaluate()\n",
    "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRDlQ0kDoWp5"
   },
   "outputs": [],
   "source": [
    "47.97"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "finetune.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
