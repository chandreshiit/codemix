{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sarc-self-match-bi-LSTM-fastext-twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandu8542/codemix/blob/main/sarc_self_match_bi_LSTM_fastext_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2wp3B-p_O7D"
      },
      "source": [
        "# ! pip install wget"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9X0QGhs3Xlm"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUZxP39u3D_f"
      },
      "source": [
        "#import wget\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18T245ybi91l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjNwXp1TfzU-"
      },
      "source": [
        "\n",
        "# import gzip\n",
        "# import shutil\n",
        "# from zipfile import ZipFile "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTheG5eRFIyA"
      },
      "source": [
        "# from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "# tf.keras.backend.clear_session()\n",
        "\n",
        "# config_proto = tf.compat.v1.ConfigProto()\n",
        "# off = rewriter_config_pb2.RewriterConfig.OFF\n",
        "# config_proto.graph_options.rewrite_options.arithmetic_optimization = off\n",
        "# config_proto.graph_options.rewrite_options.memory_optimization = off"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtjtlGBeQIbL"
      },
      "source": [
        "# tf.config.optimizer.set_experimental_options({'arithmetic_optimization': rewriter_config_pb2.RewriterConfig.OFF,\n",
        "#                                               'memory_optimization': rewriter_config_pb2.RewriterConfig.OFF})\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mq_8q3MKfSz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHboL1DByCyw"
      },
      "source": [
        "'''def load_file(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text.split(\"\\n\")'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIUwWDTDLaKM"
      },
      "source": [
        "def get_max_len(tweets):\n",
        "    max_tweet_len = len(max(tweets, key=len).split())\n",
        "    return max_tweet_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-mbmWEqoQgQ"
      },
      "source": [
        "def get_mean_len(tweets):\n",
        "    sum_of_length = sum([len(l.split()) for l in tweets])\n",
        "    avg_tweet_len = sum_of_length / float(len(tweets))\n",
        "    return avg_tweet_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgb3-bEv5ymq"
      },
      "source": [
        "'''url_common = \"https://www.dropbox.com/s/flh1fjynqvdsj4p/lexvec.commoncrawl.300d.W.pos.vectors.gz?dl=1\"\n",
        "url_wiki = \"https://www.dropbox.com/s/kguufyc2xcdi8yk/lexvec.enwiki%2Bnewscrawl.300d.W.pos.vectors.gz?dl=1\"\n",
        "\n",
        "file_commmon = 'lexvec.commoncrawl.300d.W.pos.vectors.gz'\n",
        "file_wiki = 'lexvec.enwiki+newscrawl.300d.W.pos.vectors.gz'\n",
        "\n",
        "if os.path.isfile(file_wiki[:-3]):\n",
        "    print (\"File exist\")\n",
        "else:\n",
        "    wget.download(url_wiki)  \n",
        "    with gzip.open(file_wiki, 'rb') as f_in:\n",
        "        with open(file_wiki[:-3], 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)'''\n",
        "\n",
        "df_embedding = pd.read_csv('/content/drive/MyDrive/models/hing_emb', sep=\" \", quoting=3, header=None, index_col=0,skiprows=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXPzAUnv3II9"
      },
      "source": [
        "**LexVec Word Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF6son_jXbAG"
      },
      "source": [
        "embedding_matrix = df_embedding.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK1_bVo4162H"
      },
      "source": [
        "**Extracting Vocabulary from word embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaugOfQwXnRj"
      },
      "source": [
        "vocab = []\n",
        "\n",
        "for word in list(df_embedding.index):\n",
        "  vocab.append(str(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX7oxJSNYbOM"
      },
      "source": [
        "vocab_size , vocab_dim = embedding_matrix.shape\n",
        "vocab_size, vocab_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3_tf2o-EUsz"
      },
      "source": [
        "**Reading the data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bIYFokEdWQw"
      },
      "source": [
        "traindata = pd.read_csv(\"/content/drive/MyDrive/dataset/split_dataset/train.csv\",sep=',')\n",
        "testdata = pd.read_csv(\"/content/drive/MyDrive/dataset/split_dataset/test.csv\",sep=',')\n",
        "validdata = pd.read_csv(\"/content/drive/MyDrive/dataset/split_dataset/valid.csv\",sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsqB1BdsSv2K"
      },
      "source": [
        "print(traindata.shape)\n",
        "print(testdata.shape)\n",
        "print(validdata.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NQzxiGzJxlt"
      },
      "source": [
        "X_train_data = traindata['tweets']\n",
        "y_train_labels = traindata['labels']\n",
        "X_test_data = testdata['tweets']\n",
        "y_test_labels = testdata['labels']\n",
        "X_valid_data = validdata['tweets']\n",
        "y_valid_labels  = validdata['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxNKuhoBAJmF"
      },
      "source": [
        "y_test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0tliTJ-L0Aj"
      },
      "source": [
        "max_input_length = get_max_len(X_train_data)\n",
        "mean_input_length = get_mean_len(X_train_data)\n",
        "max_input_length, int(mean_input_length)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52R91-rW1W5B"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GZ14Cics8Y6"
      },
      "source": [
        "tokenizer = Tokenizer(num_words= vocab_size)\n",
        "tokenizer.fit_on_texts(vocab)\n",
        "\n",
        "maxlen = max_input_length\n",
        "\n",
        "# tokenizing training data\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train_data)\n",
        "X_train_data = pad_sequences(sequences_train, maxlen=maxlen,padding='post',truncating='post')\n",
        "\n",
        "# # tokenizing test data\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test_data)\n",
        "X_test_data = pad_sequences(sequences_test, maxlen=maxlen,padding='post',truncating='post')\n",
        "\n",
        "# tokenize the validation data\n",
        "sequences_valid = tokenizer.texts_to_sequences(X_valid_data)\n",
        "X_valid_data = pad_sequences(sequences_valid, maxlen=maxlen,padding='post',truncating='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzF9D80qtm0j"
      },
      "source": [
        "print(sequences_train[0])\n",
        "print(X_train_data[0])\n",
        "tokenizer.sequences_to_texts([sequences_train[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxddN6xgZxbc"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/models/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r_A-VoyK9YU"
      },
      "source": [
        "'''class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    try:\n",
        "      if(logs.get('accuracy')>0.979):\n",
        "        self.model.stop_training = True\n",
        "    except:\n",
        "      if(logs.get('acc')>0.979):\n",
        "        self.model.stop_training = True\n",
        "\n",
        "\n",
        "acc_callback = myCallback()\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping()\n",
        "'''\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 5:\n",
        "    print(\"Learning rate\", lr)\n",
        "    return lr\n",
        "  else:\n",
        "    lr = lr * tf.math.exp(-0.1)\n",
        "    print(\"Epoch: {} Learning rate: {}\", epoch, lr)\n",
        "    return lr\n",
        "\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyN9dTkFNqA6"
      },
      "source": [
        "class SelfMatchingLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self,  seq_length, embed_dim, **kwargs):\n",
        "      self.seq_length = seq_length\n",
        "      self.embed_dim  = embed_dim\n",
        "      self.output_dim = seq_length*embed_dim # dim of the paramter W\n",
        "      super(SelfMatchingLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape): # input_shape is the shape of x\n",
        "      self.M = self.add_weight(name='M', # M is square matrix of shape n*k X n*k , see the paper selfmatching network\n",
        "                                      shape=tf.TensorShape((self.output_dim, self.output_dim )).as_list(),\n",
        "                                      initializer='glorot_uniform',\n",
        "                                      trainable=True)\n",
        "      super(SelfMatchingLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "      \n",
        "    def call(self,x):\n",
        "      # print(K.int_shape(x))\n",
        "      # create an identity matrix of shape seq_length\n",
        "      t = tf.eye(self.seq_length, dtype=x.dtype) \n",
        "      # Add one dimension to x and one dimension to t\n",
        "      xt = tf.expand_dims(x, 1) * tf.expand_dims(t, 2)\n",
        "      # Reshape, output is basically a diagnoal magtrix whose entries are embeddings\n",
        "      output = tf.reshape(xt, (-1, self.seq_length, self.output_dim))\n",
        "      # E*M*E^T happens bellow\n",
        "      tmp = tf.matmul(output, self.M)\n",
        "      result = tf.matmul(tmp,output, transpose_b=True)\n",
        "      # maximize by rows\n",
        "      result = tf.reduce_max(result, axis=-1)\n",
        "      # take softmax\n",
        "      result = tf.nn.softmax(result)\n",
        "      result = tf.einsum('ij,ijk->ik',result,x)\n",
        "      return result\n",
        "    def compute_output_shape(input_shape):\n",
        "      return (input_shape[0], self.output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PCAUZvgyTiz"
      },
      "source": [
        "input_layer = tf.keras.Input(shape=(maxlen, ),name='text_input')\n",
        "input_embedding_layer = tf.keras.layers.Embedding(vocab_size, vocab_dim,\n",
        "                        embeddings_initializer=Constant(embedding_matrix), \n",
        "                        trainable = False)\n",
        "bi_lstm_layer = layers.Bidirectional(layers.LSTM(100, dropout=0.4,\n",
        "                        recurrent_dropout=0.4))\n",
        "selfmatching_layer = SelfMatchingLayer(maxlen,vocab_dim, name='selfmatch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JtwRmigW9cl"
      },
      "source": [
        "embedding_output = input_embedding_layer(input_layer)\n",
        "\n",
        "selfmatch_output = selfmatching_layer(embedding_output)\n",
        "bi_lstm_output = bi_lstm_layer(embedding_output)\n",
        "\n",
        "concat_features = tf.concat([selfmatch_output, bi_lstm_output],-1)\n",
        "sarc_prediction = layers.Dense(64,activation='relu',name='activation_relu')(concat_features)\n",
        "sarc_prediction = layers.Dense(2,activation='softmax',name='activation_softmax')(sarc_prediction)\n",
        "#sarc_prediction = layers.Softmax()(sarc_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8HP_palWcmG"
      },
      "source": [
        "model = tf.keras.Model(input_layer, sarc_prediction)\n",
        "tf.keras.utils.plot_model(model,'model.jpg',show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEx0uR-v6uuw"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rk7hO9LIwcO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHP8JIFuy1Xg"
      },
      "source": [
        "history = model.fit(np.array(X_train_data), np.array(y_train_labels), validation_data=(np.array(X_valid_data), np.array(y_valid_labels)), epochs=20, callbacks= [callback]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msuPEjhTzV_O"
      },
      "source": [
        "prediction = model.predict(np.array(X_test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nryAmwh39daW"
      },
      "source": [
        "prediction[11]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F-t5PohXmFe"
      },
      "source": [
        "y_test_pred = np.argmax(prediction,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDv3ypPs9m8k"
      },
      "source": [
        "y_test_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJDH-_iH9uA8"
      },
      "source": [
        "np.array(y_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAYZcTGTkpxI"
      },
      "source": [
        "print(\"precision_score\", precision_score(y_test_labels, y_test_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTqeIQz3EyaE"
      },
      "source": [
        "print(\"recall_score\", recall_score(y_test_labels, y_test_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho9n0U5lE0UI"
      },
      "source": [
        "print(\"f1_score_micro\", f1_score(y_test_labels, y_test_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1n4XKhII8Vb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Ii_j5AE41x"
      },
      "source": [
        "print(\"f1_score_binary\", f1_score(y_test_labels, y_test_pred, average='binary'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VExnrWdpFBSz"
      },
      "source": [
        "print(\"accuracy_score\", accuracy_score(y_test_labels, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyQv_u7oFD9L"
      },
      "source": [
        "print(\"avg_precison_score\", precision_score(y_test_labels, y_test_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbXodk43Izj-"
      },
      "source": [
        "#Old data training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niq51pVhJEE4"
      },
      "source": [
        "history = model.fit(np.array(X_train_data), np.array(y_train_labels), validation_split=0.2, epochs=10,callbacks= [cp_callback]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNbPNoaRJF3V"
      },
      "source": [
        "prediction = model.predict(np.array(X_test_data))\n",
        "y_test_pred = np.argmax(prediction,axis=1)\n",
        "print(\"precision_score\", precision_score(y_test_labels, y_test_pred, average='micro'))\n",
        "print(\"recall_score\", recall_score(y_test_labels, y_test_pred, average='micro'))\n",
        "print(\"f1_score_micro\", f1_score(y_test_labels, y_test_pred, average='binary'))\n",
        "print(\"accuracy_score\", accuracy_score(y_test_labels, y_test_pred))\n",
        "\n",
        "print(y_test_pred, y_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8HGf1cjyrUm"
      },
      "source": [
        "test = [\n",
        "        \"TensorFlow Datasets is a collection of datasets ready to use with TensorFlow\",\n",
        "        \"It can be difficult for a beginner to the field of deep learning to know what type of network to use. There are so many types of networks to choose from and new methods being published and discussed every day.\",\n",
        "        \"It can be difficult for a beginner to the field of deep learning to know what type of network to use.\",\n",
        "        \"There are so many types of networks to choose from and new methods being published and discussed every day.\"\n",
        "        \"I'm glad we're having a rehearsal dinner\",\n",
        "        \"I rarely practice my meals before I eat\",\n",
        "        \"I'm glad we're having a rehearsal dinner. I rarely practice my meals before I eat\",\n",
        "        \"Thank you for explaining that my eye cancer isn't going to make me deaf\",\n",
        "        \"I feel so fortunate that an intellectual giant like yourself would deign to operate on me\",\n",
        "        \"Thank you for explaining that my eye cancer isn't going to make me deaf. I feel so fortunate that an intellectual giant like yourself would deign to operate on me.\"]\n",
        "seq_test = tokenizer.texts_to_sequences(test)\n",
        "test = pad_sequences(seq_test, maxlen=maxlen,truncating='post', padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp4mYzLYqqjE"
      },
      "source": [
        "# del model\n",
        "# # Recreate the exact same model purely from the file:\n",
        "# model = keras.models.load_model('path_to_my_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIV9D7Y-pgsw"
      },
      "source": [
        "np.argmax(model.predict([test]),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo7Q3BPlW80A"
      },
      "source": [
        "try:\n",
        "  plt.plot(history.history['acc'], label='accuracy')\n",
        "  plt.plot(history.history['val_acc'], label = 'val_accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.ylim([0.5, 1])\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.savefig('plot.png')\n",
        "except:\n",
        "  plt.plot(history.history['accuracy'], label='accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.ylim([0.5, 1])\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.savefig('plot.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLJd1R-9qfbs"
      },
      "source": [
        "model.save('self-match-bi-lstm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phQX9Nww5Hyv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}